# Configuration for a local AI provider (e.g., Ollama, LM Studio)
embedding:
  api_url: "${EMBEDDINGS_API_URL}"
  model_name: "text-embedding-qwen3-embedding-8b"

providers:
  gemini_default:
    provider: "gemini"
    api_url: "${AI_API_URL}"
    api_key: "${AI_API_KEY}"
    model_name: "gemini-2.5-flash-lite"

  local_default:
    provider: "local"
    api_url: "http://localhost:1234/v1/chat/completions"
    api_key: null
    model_name: "qwen3-coder-30b-a3b-instruct-mlx"

# Override all default tasks to use the local provider.
# The prompts for these tasks are inherited from `config.prompt.yml`
# unless you create a `prompt.yml` file to override them.
tasks:
  query_generation:
    provider: "local_default"
  direct_generation:
    provider: "local_default"
  rag_synthesis:
    provider: "local_default"
  knowledge_distillation:
    provider: "local_default"
  query_analysis:
    provider: "local_default"
  llm_rerank:
    provider: "local_default"
  knowledge_augmentation:
    provider: "local_default"
  knowledge_metadata_extraction:
    provider: "local_default"
